{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1218,"status":"ok","timestamp":1699217337048,"user":{"displayName":"Georgii Budnik","userId":"14371656724304164452"},"user_tz":-120},"id":"_B7Dl6A7RB0L","outputId":"e8317872-6483-41b9-8035-a9b44093d1b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'nlp-Text-De-toxification'...\n","remote: Enumerating objects: 126, done.\u001b[K\n","remote: Counting objects: 100% (36/36), done.\u001b[K\n","remote: Compressing objects: 100% (35/35), done.\u001b[K\n","remote: Total 126 (delta 11), reused 0 (delta 0), pack-reused 90\u001b[K\n","Receiving objects: 100% (126/126), 2.02 MiB | 16.44 MiB/s, done.\n","Resolving deltas: 100% (42/42), done.\n"]}],"source":["import os\n","\n","repo_dir = \"nlp-Text-De-toxification\"\n","\n","if os.path.exists(repo_dir):\n","    print(f\"{repo_dir} already exists. Removing it...\\n\")\n","    !rm -r {repo_dir}\n","\n","# Clone the repository from GitHub\n","!git clone https://github.com/Goshmar/nlp-Text-De-toxification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"NA7AfrR8Re3V"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.4/677.4 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.8/400.8 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.7/508.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sentencepiece (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for importlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dask 2023.8.1 requires importlib-metadata\u003e=4.13.0, but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["! pip install -r nlp-Text-De-toxification/requirements.txt -q"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PQxQYGhTTxqV"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"\u003cipython-input-3-0bb693ff40af\u003e\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    import condbert\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["import sys\n","sys.path.append('/content/nlp-Text-De-toxification/src/models')\n","\n","from importlib import reload\\\n","import condbert\n","from condbert import CondBertRewriter\n","reload(condbert)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5Wz9U62PVHUx"},"outputs":[],"source":["import masked_token_predictor_bert\n","reload(masked_token_predictor_bert)\n","from masked_token_predictor_bert import MaskedTokenPredictorBert"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hH2lPq-NVJAk"},"outputs":[],"source":["import choosers\n","reload(choosers)\n","from choosers import EmbeddingSimilarityChooser"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"de1Dc4vSUJUW"},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","import pickle\n","from tqdm.auto import tqdm, trange"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Em7qW0AwUKb-"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # please change it to e.g. 'cuda:0' if you want to use a GPU\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XxXoGIxpUcZF"},"outputs":[],"source":["model_name = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForMaskedLM.from_pretrained(model_name)\n","model.to(device);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"B4GWeHnPRgVJ"},"outputs":[],"source":["# Downloading vocab for CondBert model\n","def load_words_from_file(file_path):\n","    with open(file_path, \"r\") as file:\n","        words = [line.strip() for line in file.readlines()]\n","    return words\n","\n","words_root = \"/content/nlp-Text-De-toxification/data/external/\"\n","negative_words = load_words_from_file(words_root + \"negative-words.txt\")\n","positive_words = load_words_from_file(words_root + \"positive-words.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6kudsIcSkdz"},"outputs":[],"source":["import pickle\n","\n","interim_root = '/content/nlp-Text-De-toxification/data/interim/'\n","\n","with open(interim_root + \"/word2coef.pkl\", 'rb') as f:\n","    word2coef = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cDbCv3tTIIl"},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","import numpy as np\n","import pickle\n","from tqdm.auto import tqdm, trange\n","\n","import numpy as np\n","import pandas as pd\n","\n","token_toxicities = []\n","with open(interim_root + \"token_toxicities.txt\", 'r') as f:\n","    for line in f.readlines():\n","        token_toxicities.append(float(line))\n","token_toxicities = np.array(token_toxicities)\n","token_toxicities = np.maximum(0, np.log(1/(1/token_toxicities-1)))   # log odds ratio\n","# discourage meaningless tokens\n","for tok in ['.', ',', '-']:\n","    token_toxicities[tokenizer.encode(tok)][1] = 3\n","\n","for tok in ['you']:\n","    token_toxicities[tokenizer.encode(tok)][1] = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mr8UFZuTcNN"},"outputs":[],"source":["reload(condbert)\n","\n","editor = CondBertRewriter(\n","    model=model,\n","    tokenizer=tokenizer,\n","    device=device,\n","    neg_words=negative_words,\n","    pos_words=positive_words,\n","    word2coef=word2coef,\n","    token_toxicities=token_toxicities,\n","    predictor=None,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9n5aAAbbUtnS"},"outputs":[],"source":["predictor = MaskedTokenPredictorBert(model, tokenizer, max_len=250, device=device, label=0, contrast_penalty=0.0)\n","editor.predictor = predictor\n","\n","def adjust_logits(logits, label):\n","    return logits - editor.token_toxicities * 5\n","\n","predictor.logits_postprocessor = adjust_logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68VYM2tfYTdl"},"outputs":[],"source":["chooser = EmbeddingSimilarityChooser(sim_coef=10, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45663,"status":"ok","timestamp":1698163151265,"user":{"displayName":"Georgii Budnik","userId":"14371656724304164452"},"user_tz":-180},"id":"vN-DBKICVLWd","outputId":"da947c05-e999-4643-f178-dfedefa911be"},"outputs":[{"name":"stdout","output_type":"stream","text":["------\n","Yeah, you fuckin' cow. No one in this country can ever pronounce my name right.\n","--\u003e yeah , you ' re goin ' cow . no one in this country can ever pronounce my name right .\n","------\n","\n","\n","------\n","Do you really want to explain it to every asshole who orders a drink?\n","--\u003e do you really want to explain it to every sane person who orders a drink ?\n","------\n","\n","\n","------\n","'Grinning Monkey', Argo called him and Lin-tse had despised the youth for his cowardice.\n","--\u003e ' grinning like a rabbit ' , argo called him and lin - tse had once mocked the youth for his \" mischief \" .\n","------\n","\n","\n"]}],"source":["dataset = pd.read_csv(\"/content/nlp-Text-De-toxification/data/interim/dataset_cropped.csv\")\n","for example in dataset['reference'].sample(3):\n","    print(\"------\")\n","    print(example)\n","    print(\"--\u003e\", editor.replacement_loop(example, verbose=False, chooser=chooser, n_tokens=(1, 2, 3), n_top=10))\n","    print(\"------\\n\\n\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMSvMinLZd7CnNRSOInNSHn","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}