# -*- coding: utf-8 -*-
"""train-predict_condbert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ob5Nc1PdhP8esAbDIZ_A01J9AF4xYXs-
"""

import os

repo_dir = "nlp-Text-De-toxification"

if os.path.exists(repo_dir):
    print(f"{repo_dir} already exists. Removing it...\n")
    !rm -r {repo_dir}

# Clone the repository from GitHub
!git clone https://github.com/Goshmar/nlp-Text-De-toxification

! pip install -r nlp-Text-De-toxification/requirements.txt -q

import sys
sys.path.append('/content/nlp-Text-De-toxification/src/models')

from importlib import reload
import condbert
from condbert import CondBertRewriter
reload(condbert)

import masked_token_predictor_bert
reload(masked_token_predictor_bert)
from masked_token_predictor_bert import MaskedTokenPredictorBert

import choosers
reload(choosers)
from choosers import EmbeddingSimilarityChooser

import torch
from transformers import BertTokenizer, BertForMaskedLM
import pickle
from tqdm.auto import tqdm, trange

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # please change it to e.g. 'cuda:0' if you want to use a GPU
device

model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForMaskedLM.from_pretrained(model_name)
model.to(device);

# Downloading vocab for CondBert model
def load_words_from_file(file_path):
    with open(file_path, "r") as file:
        words = [line.strip() for line in file.readlines()]
    return words

words_root = "/content/nlp-Text-De-toxification/data/external/"
negative_words = load_words_from_file(words_root + "negative-words.txt")
positive_words = load_words_from_file(words_root + "positive-words.txt")

import pickle

interim_root = '/content/nlp-Text-De-toxification/data/interim/'

with open(interim_root + "/word2coef.pkl", 'rb') as f:
    word2coef = pickle.load(f)

import torch
from transformers import BertTokenizer, BertForMaskedLM
import numpy as np
import pickle
from tqdm.auto import tqdm, trange

import numpy as np
import pandas as pd

token_toxicities = []
with open(interim_root + "token_toxicities.txt", 'r') as f:
    for line in f.readlines():
        token_toxicities.append(float(line))
token_toxicities = np.array(token_toxicities)
token_toxicities = np.maximum(0, np.log(1/(1/token_toxicities-1)))   # log odds ratio
# discourage meaningless tokens
for tok in ['.', ',', '-']:
    token_toxicities[tokenizer.encode(tok)][1] = 3

for tok in ['you']:
    token_toxicities[tokenizer.encode(tok)][1] = 0

reload(condbert)

editor = CondBertRewriter(
    model=model,
    tokenizer=tokenizer,
    device=device,
    neg_words=negative_words,
    pos_words=positive_words,
    word2coef=word2coef,
    token_toxicities=token_toxicities,
    predictor=None,
)

predictor = MaskedTokenPredictorBert(model, tokenizer, max_len=250, device=device, label=0, contrast_penalty=0.0)
editor.predictor = predictor

def adjust_logits(logits, label):
    return logits - editor.token_toxicities * 5

predictor.logits_postprocessor = adjust_logits

chooser = EmbeddingSimilarityChooser(sim_coef=10, tokenizer=tokenizer)

dataset = pd.read_csv("/content/nlp-Text-De-toxification/data/interim/dataset_cropped.csv")
for example in dataset['reference'].sample(3):
    print("------")
    print(example)
    print("-->", editor.replacement_loop(example, verbose=False, chooser=chooser, n_tokens=(1, 2, 3), n_top=10))
    print("------\n\n")