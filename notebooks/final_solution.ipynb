{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","\n","repo_dir = \"nlp-Text-De-toxification\"\n","\n","if os.path.exists(repo_dir):\n","    print(f\"{repo_dir} already exists. Removing it...\\n\")\n","    !rm -r {repo_dir}\n","\n","# Clone the repository from GitHub\n","!git clone https://github.com/Goshmar/nlp-Text-De-toxification"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gXHJPAHKvAF","outputId":"f908743b-9b85-4b43-9bb7-9eaf4b883b20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nlp-Text-De-toxification already exists. Removing it...\n","\n","Cloning into 'nlp-Text-De-toxification'...\n","remote: Enumerating objects: 118, done.\u001b[K\n","remote: Counting objects: 100% (28/28), done.\u001b[K\n","remote: Compressing objects: 100% (27/27), done.\u001b[K\n","remote: Total 118 (delta 7), reused 0 (delta 0), pack-reused 90\u001b[K\n","Receiving objects: 100% (118/118), 2.02 MiB | 11.24 MiB/s, done.\n","Resolving deltas: 100% (38/38), done.\n"]}]},{"cell_type":"code","source":["\n","! pip install -r nlp-Text-De-toxification/requirements.txt -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-a0cH44K1QN","outputId":"70dc5e0d-73d3-47b1-ff8c-0e5b916d4d9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.4/677.4 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.8/400.8 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.7/508.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sentencepiece (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for importlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dask 2023.8.1 requires importlib-metadata>=4.13.0, but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import pandas as pd\n","import requests\n","import zipfile\n","\n","# Define the paths\n","dataset_url = \"https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\"\n","zip_file_path = \"dataset.zip\"\n","csv_file_path, tsv_file_path = \"dataset.csv\", \"filtered.tsv\"\n","\n","# Download the ZIP file\n","response = requests.get(dataset_url)\n","if response.status_code == 200:\n","    with open(zip_file_path, 'wb') as file:\n","        file.write(response.content)\n","else:\n","    print(\"Attempt failed\")\n","    exit()\n","\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extractall(\".\")\n","\n","dataset = pd.read_csv(\"filtered.tsv\", delimiter='\\t')\n","dataset.to_csv(csv_file_path, index=False)\n","\n","# ZIP cleaning up\n","os.remove(zip_file_path)\n","os.remove(tsv_file_path)"],"metadata":{"id":"a6Aa8iqEuenQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Split the dataset into training and validation sets\n","train_data, val_data = train_test_split(dataset, test_size=0.1, random_state=42)\n","\n","# Define the T5 model and tokenizer\n","model_name = \"t5-small\"\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name)"],"metadata":{"id":"B8FbgjUVAbLZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize and preprocess the data\n","def preprocess_data(data):\n","    input_text = data['reference'].apply(lambda x: \"detoxify: \" + x + \" </s>\")\n","    target_text = data['translation'].apply(lambda x: x + \" </s>\")\n","\n","    input_text = list(input_text)\n","    target_text = list(target_text)\n","\n","    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","    targets = tokenizer(target_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","\n","    return inputs, targets\n","\n","train_inputs, train_targets = preprocess_data(train_data)\n","val_inputs, val_targets = preprocess_data(val_data)"],"metadata":{"id":"iwu5wtUdFeGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function for training the model\n","def train(model, train_inputs, train_targets, val_inputs, val_targets, num_epochs=5, batch_size=32, learning_rate=1e-4):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","\n","        model.train()\n","        for i in range(0, len(train_inputs['input_ids']), batch_size):\n","            input_batch = {key: value[i:i+batch_size].to(device) for key, value in train_inputs.items()}\n","            target_batch = {key: value[i:i+batch_size].to(device) for key, value in train_targets.items()}\n","\n","            optimizer.zero_grad()\n","            loss = model(**input_batch, labels=target_batch['input_ids']).loss\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0.0\n","            for i in range(0, len(val_inputs['input_ids']), batch_size):\n","                input_batch = {key: value[i:i+batch_size].to(device) for key, value in val_inputs.items()}\n","                target_batch = {key: value[i:i+batch_size].to(device) for key, value in val_targets.items()}\n","\n","                loss = model(**input_batch, labels=target_batch['input_ids']).loss\n","                val_loss += loss.item()\n","\n","        val_loss /= (len(val_inputs['input_ids']) / batch_size)\n","        print(f\"Validation Loss: {val_loss:.4f}\")\n","\n","# Train the model\n","train(model, train_inputs, train_targets, val_inputs, val_targets, num_epochs=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WmByo70VCow9","outputId":"7f84fb8d-eb70-49a7-ccb2-c63cfee52b7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","Validation Loss: 1.6122\n","Epoch 2/20\n","Validation Loss: 1.5038\n","Epoch 3/20\n","Validation Loss: 1.4839\n","Epoch 4/20\n","Validation Loss: 1.4191\n","Epoch 5/20\n","Validation Loss: 1.3536\n","Epoch 6/20\n","Validation Loss: 1.2999\n","Epoch 7/20\n","Validation Loss: 1.2604\n","Epoch 8/20\n","Validation Loss: 1.1958\n","Epoch 9/20\n","Validation Loss: 1.1483\n","Epoch 10/20\n","Validation Loss: 1.1075\n","Epoch 11/20\n","Validation Loss: 1.0806\n","Epoch 12/20\n","Validation Loss: 1.0617\n","Epoch 13/20\n","Validation Loss: 1.0501\n","Epoch 14/20\n","Validation Loss: 1.0404\n","Epoch 15/20\n","Validation Loss: 1.0361\n","Epoch 16/20\n","Validation Loss: 1.0318\n","Epoch 17/20\n","Validation Loss: 1.0294\n","Epoch 18/20\n","Validation Loss: 1.0297\n","Epoch 19/20\n","Validation Loss: 1.0313\n","Epoch 20/20\n","Validation Loss: 1.0296\n"]}]},{"cell_type":"code","source":["# Detoxify a sample sentence\n","def detoxify_sentence(sentence):\n","    input_text = \"detoxify: \" + sentence + \" </s>\"\n","    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=128).input_ids.to(model.device)\n","\n","    with torch.no_grad():\n","        output_ids = model.generate(input_ids, max_length=128, num_return_sequences=1, no_repeat_ngram_size=2)\n","\n","    detoxified_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return detoxified_text\n","\n","dataset = pd.read_csv(\"/content/nlp-Text-De-toxification/data/interim/dataset_cropped.csv\")\n","for example in dataset['reference'].sample(3):\n","    print(\"------\")\n","    print(example)\n","    print(\"-->\", detoxify_sentence(example))\n","    print(\"------\\n\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-s_D_3VXJXDx","outputId":"66bb7071-90e7-46dd-aa9d-e574d309ced1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["------\n","Great idea, yeah, Cyril. Let's give an M-16 to a bunch of wild Indians!\n","--> great idea, yeah, cyril. we're going to give a bunch of wild Indians a sixteenth.\n","------\n","\n","\n","------\n","Looks like she left in a hurry, or she's just a filthy pig.\n","--> looks like she left in a hurry, or she's just a a terrible mess.\n","------\n","\n","\n","------\n","You're stepping on it! - Shut up.\n","--> be out! you're stepping on it!\n","------\n","\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"e4PuUFHSLWV0"},"execution_count":null,"outputs":[]}]}